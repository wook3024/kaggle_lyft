{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kaggle_lyft_tpu_prediction_single.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njc7JyU5OKL8"
      },
      "source": [
        "# function ClickConnect(){\n",
        "# console.log(\"Working\"); \n",
        "# document.querySelector(\"colab-toolbar-button\").click() \n",
        "# }setInterval(ClickConnect, 1800000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "285IhVH_o-ke"
      },
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgLvBRvPALn4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2d76640-881d-447e-c31d-9b25b38ee7d1"
      },
      "source": [
        "os.makedirs('/content/lyft-motion-prediction-autonomous-vehicles/', exist_ok=True)\n",
        "os.chdir('/content/lyft-motion-prediction-autonomous-vehicles/')\n",
        "\n",
        "%cd '/content/lyft-motion-prediction-autonomous-vehicles/'\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/lyft-motion-prediction-autonomous-vehicles\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqUrrU92ANO_"
      },
      "source": [
        "!pip uninstall -y kaggle\n",
        "!pip install --upgrade pip \n",
        "!pip install kaggle==1.5.6\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp ../kaggle.json ~/.kaggle/\n",
        "\n",
        "!kaggle competitions download -c lyft-motion-prediction-autonomous-vehicles\n",
        "!unzip lyft-motion-prediction-autonomous-vehicles.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07oqcbtHpRTt"
      },
      "source": [
        "# import os\n",
        "# os.makedirs('/content/lyft-motion-prediction-autonomous-vehicles/', exist_ok=True)\n",
        "# os.chdir('/content/lyft-motion-prediction-autonomous-vehicles/')\n",
        "%cd '/content/lyft-motion-prediction-autonomous-vehicles/'\n",
        "\n",
        "!pip install --target=/content/lyft-motion-prediction-autonomous-vehicles pymap3d==2.1.0 -q\n",
        "!pip install --target=/content/lyft-motion-prediction-autonomous-vehicles strictyaml -q\n",
        "!pip install --target=/content/lyft-motion-prediction-autonomous-vehicles protobuf==3.12.2 -q\n",
        "!pip install --target=/content/lyft-motion-prediction-autonomous-vehicles transforms3d -q\n",
        "!pip install --target=/content/lyft-motion-prediction-autonomous-vehicles zarr -q\n",
        "!pip install --target=/content/lyft-motion-prediction-autonomous-vehicles ptable -q\n",
        "!pip install --no-dependencies --target=/content/lyft-motion-prediction-autonomous-vehicles l5kit==1.1.0 --upgrade -q\n",
        "\n",
        "# !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.6-cp36-cp36m-linux_x86_64.whl\n",
        "VERSION = \"nightly\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION\n",
        "\n",
        "!pip install pytorch-lightning==1.0.4\n",
        "!pip install adamp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hgZjwgZCySk"
      },
      "source": [
        "# !pip install l5kit\n",
        "# !pip install adamp\n",
        "# !pip install pytorch-lightning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv3TuFB0AY_9"
      },
      "source": [
        "INPUT_DIR = '/content/lyft-motion-prediction-autonomous-vehicles/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHk6kk_9AahV"
      },
      "source": [
        "from __future__ import print_function, division, absolute_import\n",
        "from typing import Dict\n",
        "\n",
        "from tempfile import gettempdir\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models.resnet import resnet50, resnet18, resnet34, resnet101\n",
        "from tqdm import tqdm\n",
        "\n",
        "import l5kit\n",
        "from l5kit.configs import load_config_data\n",
        "from l5kit.data import LocalDataManager, ChunkedDataset\n",
        "from l5kit.dataset import AgentDataset, EgoDataset\n",
        "from l5kit.rasterization import build_rasterizer\n",
        "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
        "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
        "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
        "from l5kit.geometry import transform_points\n",
        "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
        "from prettytable import PrettyTable\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "\n",
        "from collections import OrderedDict\n",
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.utils import model_zoo\n",
        "from adamp import AdamP\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning import LightningModule\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "\n",
        "import gc\n",
        "\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# l5kit.__version__\n",
        "\n",
        "# def set_seed(seed):\n",
        "#     random.seed(seed)\n",
        "#     np.random.seed(seed)\n",
        "#     os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "#     torch.manual_seed(seed)\n",
        "#     # torch.cuda.manual_seed(seed)\n",
        "    \n",
        "# set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bW_MQFEAzIb"
      },
      "source": [
        "cfg = {\n",
        "    'format_version': 4,\n",
        "    'data_path': \"/content/lyft-motion-prediction-autonomous-vehicles\",\n",
        "    'model_params': {\n",
        "        'model_architecture': 'resnet18',\n",
        "        'history_num_frames': 10,\n",
        "        'history_step_size': 1,\n",
        "        'history_delta_time': 0.1,\n",
        "        'future_num_frames': 50,\n",
        "        'future_step_size': 1,\n",
        "        'future_delta_time': 0.1,\n",
        "        'model_name': \"resnest50+267+adamp+0.5+10+1e-4+yaw_head3\",\n",
        "        # 'model_name': \"lr_finder_test\",\n",
        "        'lr': 1e-4,\n",
        "        # 'weight_path': \"/content/lyft-motion-prediction-autonomous-vehicles/pytorch_lightning-models-v21.ckpt\",\n",
        "        # 'weight_path': \"./model_resnet34_output_0.pth\",\n",
        "        'weight_path': None,\n",
        "        'train': True,\n",
        "        'predict': True,\n",
        "    },\n",
        "\n",
        "    'raster_params': {\n",
        "        'raster_size': [267, 267],\n",
        "        'pixel_size': [0.5, 0.5],\n",
        "        'ego_center': [0.25, 0.5],\n",
        "        'map_type': 'py_semantic',\n",
        "        'satellite_map_key': 'aerial_map/aerial_map.png',\n",
        "        'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
        "        'dataset_meta_key': 'meta.json',\n",
        "        'filter_agents_threshold': 0.5,\n",
        "        'disable_traffic_light_faces': False\n",
        "    },\n",
        "\n",
        "    'train_data_loader': {\n",
        "        'key': 'scenes/sample.zarr',\n",
        "        'batch_size': 32,\n",
        "        'shuffle': True,\n",
        "        'num_workers': 8\n",
        "    },\n",
        "    \n",
        "    'test_data_loader': {\n",
        "        'key': 'scenes/test.zarr',\n",
        "        'batch_size': 32,\n",
        "        'shuffle': False,\n",
        "        'num_workers': 8\n",
        "    },\n",
        "    'val_data_loader': {\n",
        "        'key': 'scenes/validate.zarr',\n",
        "        'batch_size': 32,\n",
        "        'shuffle': False,\n",
        "        'num_workers': 8\n",
        "    },\n",
        "\n",
        "    'train_params': {\n",
        "        'max_num_steps': 101,\n",
        "        'checkpoint_every_n_steps': 20,\n",
        "    },\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDpNdavpA_CW"
      },
      "source": [
        "# --- Function utils ---\n",
        "# Original code from https://github.com/lyft/l5kit/blob/20ab033c01610d711c3d36e1963ecec86e8b85b6/l5kit/l5kit/evaluation/metrics.py\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "def pytorch_neg_multi_log_likelihood_batch(\n",
        "    gt: Tensor, pred: Tensor, confidences: Tensor, avails: Tensor\n",
        ") -> Tensor:\n",
        "    \"\"\"\n",
        "    Compute a negative log-likelihood for the multi-modal scenario.\n",
        "    log-sum-exp trick is used here to avoid underflow and overflow, For more information about it see:\n",
        "    https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations\n",
        "    https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
        "    https://leimao.github.io/blog/LogSumExp/\n",
        "    Args:\n",
        "        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n",
        "        pred (Tensor): array of shape (bs)x(modes)x(time)x(2D coords)\n",
        "        confidences (Tensor): array of shape (bs)x(modes) with a confidence for each mode in each sample\n",
        "        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n",
        "    Returns:\n",
        "        Tensor: negative log-likelihood for this example, a single float number\n",
        "    \"\"\"\n",
        "    assert len(pred.shape) == 4, f\"expected 3D (MxTxC) array for pred, got {pred.shape}\"\n",
        "    batch_size, num_modes, future_len, num_coords = pred.shape\n",
        "\n",
        "    assert gt.shape == (batch_size, future_len, num_coords), f\"expected 2D (Time x Coords) array for gt, got {gt.shape}\"\n",
        "    assert confidences.shape == (batch_size, num_modes), f\"expected 1D (Modes) array for gt, got {confidences.shape}\"\n",
        "    assert torch.allclose(torch.sum(confidences, dim=1), confidences.new_ones((batch_size,))), \"confidences should sum to 1\"\n",
        "    assert avails.shape == (batch_size, future_len), f\"expected 1D (Time) array for gt, got {avails.shape}\"\n",
        "    # assert all data are valid\n",
        "    assert torch.isfinite(pred).all(), \"invalid value found in pred\"\n",
        "    assert torch.isfinite(gt).all(), \"invalid value found in gt\"\n",
        "    assert torch.isfinite(confidences).all(), \"invalid value found in confidences\"\n",
        "    assert torch.isfinite(avails).all(), \"invalid value found in avails\"\n",
        "\n",
        "    # convert to (batch_size, num_modes, future_len, num_coords)\n",
        "    gt = torch.unsqueeze(gt, 1)  # add modes\n",
        "    avails = avails[:, None, :, None]  # add modes and cords\n",
        "\n",
        "    # error (batch_size, num_modes, future_len)\n",
        "    error = torch.sum(((gt - pred) * avails) ** 2, dim=-1)  # reduce coords and use availability\n",
        "\n",
        "    with np.errstate(divide=\"ignore\"):  # when confidence is 0 log goes to -inf, but we're fine with it\n",
        "        # error (batch_size, num_modes)\n",
        "        error = torch.log(confidences) - 0.5 * torch.sum(error, dim=-1)  # reduce time\n",
        "\n",
        "    # use max aggregator on modes for numerical stability\n",
        "    # error (batch_size, num_modes)\n",
        "    max_value, _ = error.max(dim=1, keepdim=True)  # error are negative at this point, so max() gives the minimum one\n",
        "    error = -torch.log(torch.sum(torch.exp(error - max_value), dim=-1, keepdim=True)) - max_value  # reduce modes\n",
        "    # print(\"error\", error)\n",
        "    return torch.mean(error)\n",
        "\n",
        "\n",
        "def pytorch_neg_multi_log_likelihood_single(\n",
        "    gt: Tensor, pred: Tensor, avails: Tensor\n",
        ") -> Tensor:\n",
        "    \"\"\"\n",
        "\n",
        "    Args:\n",
        "        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n",
        "        pred (Tensor): array of shape (bs)x(time)x(2D coords)\n",
        "        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n",
        "    Returns:\n",
        "        Tensor: negative log-likelihood for this example, a single float number\n",
        "    \"\"\"\n",
        "    # pred (bs)x(time)x(2D coords) --> (bs)x(mode=1)x(time)x(2D coords)\n",
        "    # create confidence (bs)x(mode=1)\n",
        "    batch_size, future_len, num_coords = pred.shape\n",
        "    confidences = pred.new_ones((batch_size, 1))\n",
        "    return pytorch_neg_multi_log_likelihood_batch(gt, pred.unsqueeze(1), confidences, avails)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x9NaKeDBBAT"
      },
      "source": [
        "class LyftMultiModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lr = cfg[\"model_params\"][\"lr\"]\n",
        "        self.num_modes = 3\n",
        "        model_name = cfg[\"model_params\"][\"model_name\"]\n",
        "\n",
        "\n",
        "        architecture = cfg[\"model_params\"][\"model_architecture\"]\n",
        "        # backbone = torch.hub.load('zhanghang1989/ResNeSt', 'resnest50', pretrained=True)\n",
        "        backbone = eval(architecture)(pretrained=True, progress=True)\n",
        "        \n",
        "        self.backbone = backbone\n",
        "\n",
        "        num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
        "        num_in_channels = 3 + num_history_channels\n",
        "        # print(self.backbone.conv1[0])\n",
        "        if architecture == \"resnest50\" or architecture == \"resnest101\":\n",
        "          self.backbone.conv1[0] = nn.Conv2d(\n",
        "              num_in_channels,\n",
        "              # self.backbone.conv1[0].out_channels,\n",
        "              32,\n",
        "              # kernel_size=self.backbone.conv1[0].kernel_size,\n",
        "              kernel_size=(3,3),\n",
        "              # stride=self.backbone.conv1[0].stride,\n",
        "              stride=(2,2),\n",
        "              # padding=self.backbone.conv1[0].padding,\n",
        "              padding=(1,1),\n",
        "              bias=False,\n",
        "          )\n",
        "        else:\n",
        "            self.backbone.conv1 = nn.Conv2d(\n",
        "                num_in_channels,\n",
        "                self.backbone.conv1.out_channels,\n",
        "                kernel_size=self.backbone.conv1.kernel_size,\n",
        "                stride=self.backbone.conv1.stride,\n",
        "                padding=self.backbone.conv1.padding,\n",
        "                bias=False,\n",
        "            )\n",
        "\n",
        "        if architecture == \"resnet50\" or architecture == \"resnext50\" or architecture == \"resnest50\" or architecture == \"resnext101\":\n",
        "            backbone_out_features = 2048\n",
        "        else:\n",
        "            backbone_out_features = 512\n",
        "\n",
        "\n",
        "        self.future_len = cfg[\"model_params\"][\"future_num_frames\"]\n",
        "        num_targets = 2 * self.future_len\n",
        "\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            # nn.Dropout(0.2),\n",
        "            nn.Linear(in_features=backbone_out_features, out_features=4096),\n",
        "            # nn.Linear(in_features=4096, out_features=1024),\n",
        "            # nn.Linear(in_features=1024, out_features=256)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.num_preds = num_targets * self.num_modes\n",
        "\n",
        "\n",
        "        self.x_preds = nn.Linear(4096, out_features=self.num_preds)\n",
        "        self.x_modes = nn.Linear(4096, out_features=self.num_modes)\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, batch):\n",
        "        x = self.backbone.conv1(batch)\n",
        "        x = self.backbone.bn1(x)\n",
        "        x = self.backbone.relu(x)\n",
        "        x = self.backbone.maxpool(x)\n",
        "\n",
        "        x = self.backbone.layer1(x)\n",
        "        x = self.backbone.layer2(x)\n",
        "        x = self.backbone.layer3(x)\n",
        "        x = self.backbone.layer4(x)\n",
        "\n",
        "        x = self.backbone.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        # yaws = torch.abs(yaws)\n",
        "        # yaws_norm = torch.div(yaw+1e-10, 7.079)\n",
        "        # for i in range(yaws.shape[0]):\n",
        "          # x[i] = torch.mul(x[i], yaws_norm[i])\n",
        "        x = self.head(x)\n",
        "\n",
        "        preds = self.x_preds(x)\n",
        "        modes = self.x_modes(x)\n",
        "        return preds, modes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dahKk9TCBKcp"
      },
      "source": [
        "def forward(data, model, device, criterion = pytorch_neg_multi_log_likelihood_batch):\n",
        "    inputs = data[\"image\"].to(device)\n",
        "    target_availabilities = data[\"target_availabilities\"].to(device)\n",
        "    targets = data[\"target_positions\"].to(device)\n",
        "    # Forward pass\n",
        "    preds, modes = model(inputs)\n",
        "    ############################################################################\n",
        "    bs, _ = preds.shape\n",
        "    preds = preds.view(bs, 3, 50, 2)\n",
        "    assert modes.shape == (bs, 3)\n",
        "    confidences = torch.softmax(modes, dim=1)\n",
        "    ############################################################################\n",
        "    loss = criterion(targets, preds, confidences, target_availabilities)\n",
        "    return loss, preds, confidences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5eJEfN9A28o"
      },
      "source": [
        "DIR_INPUT = cfg[\"data_path\"]\n",
        "os.environ[\"L5KIT_DATA_FOLDER\"] = DIR_INPUT\n",
        "dm = LocalDataManager(None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsRyf0m2pYQl"
      },
      "source": [
        "def simple_map_fn(index, flags):\n",
        "  torch.manual_seed(1234)\n",
        "\n",
        "  device = xm.xla_device()  \n",
        "\n",
        "  # if xm.is_master_ordinal():\n",
        "  t = torch.randn((2, 2), device=device)\n",
        "  # out = str(t)\n",
        "\n",
        "  # if xm.is_master_ordinal():\n",
        "  #   print(out)\n",
        "  \n",
        "  print(\"Process\", index ,\"is using\", xm.xla_real_devices([str(device)])[0])\n",
        "\n",
        "  xm.rendezvous('init')\n",
        "\n",
        "flags = {}\n",
        "xmp.spawn(simple_map_fn, args=(flags,), nprocs=8, start_method='fork')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yrB8CUBqHjA"
      },
      "source": [
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import time\n",
        "\n",
        "\n",
        "def map_fn(index, flags):\n",
        "  torch.manual_seed(flags['seed'])\n",
        "\n",
        "  device = xm.xla_device()\n",
        "\n",
        "  if not xm.is_master_ordinal():\n",
        "    xm.rendezvous('download_only_once')\n",
        "\n",
        "  test_cfg = cfg[\"test_data_loader\"]\n",
        "  rasterizer = build_rasterizer(cfg, dm)\n",
        "  test_zarr = ChunkedDataset(dm.require(test_cfg[\"key\"])).open()\n",
        "  test_dataset = AgentDataset(cfg, test_zarr, rasterizer)\n",
        "\n",
        "  if xm.is_master_ordinal():\n",
        "    xm.rendezvous('download_only_once')\n",
        "\n",
        "  test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    test_dataset,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=True\n",
        "  )\n",
        "\n",
        "\n",
        "  test_loader = DataLoader(\n",
        "      test_dataset, \n",
        "      batch_size=flags['batch_size'],  \n",
        "      sampler=test_sampler, \n",
        "      num_workers=flags['num_workers'],\n",
        "      drop_last=True)\n",
        "  \n",
        "\n",
        "  net = LyftMultiModel().to(device)\n",
        "  loss_fn = pytorch_neg_multi_log_likelihood_batch\n",
        "  optimizer = AdamP(net.parameters(), lr=0.0001, betas=(0.9, 0.999), weight_decay=1e-2)\n",
        "\n",
        "\n",
        "  num_modes = 3\n",
        "  future_len = 50\n",
        "  loader_len = len(train_loader)\n",
        "  \n",
        "  # for epoch in range(flags['num_epochs']):\n",
        "\n",
        "\n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "    para_val_loader = pl.ParallelLoader(val_loader, [device]).per_device_loader(device)\n",
        "    for batch_num, batch in enumerate(para_val_loader):\n",
        "      # loss, _, _ = forward(batch, device, net)\n",
        "      inputs = batch[\"image\"].to(device)\n",
        "      target_availabilities = batch[\"target_availabilities\"].to(device)\n",
        "      targets = batch[\"target_positions\"].to(device)\n",
        "      # Forward pass\n",
        "      preds, modes = net(inputs)\n",
        "      ############################################################################\n",
        "      bs, _ = preds.shape\n",
        "      preds = preds.view(bs, 3, 50, 2)\n",
        "      assert modes.shape == (bs, 3)\n",
        "      confidences = torch.softmax(modes, dim=1)\n",
        "      ############################################################################\n",
        "      loss = loss_fn(targets, preds, confidences, target_availabilities)\n",
        "      # images = batch[\"image\"]\n",
        "      # yaws = batch[\"yaw\"]\n",
        "      # target_availabilities = batch[\"target_availabilities\"]\n",
        "      # targets = batch[\"target_positions\"]\n",
        "\n",
        "      # pred, confidences = net(images, yaws)\n",
        "      # bs, _ = pred.shape\n",
        "      # preds = pred.view(bs, num_modes, future_len, 2)\n",
        "      # assert confidences.shape == (bs, num_modes)\n",
        "      # confidences = torch.softmax(confidences, dim=1)\n",
        "      # loss = loss_fn(targets, preds, confidences, target_availabilities)\n",
        "\n",
        "      losses.append(loss)\n",
        "\n",
        "    times.append(elapsed_train_time)\n",
        "    print(\"Process\", index, \"loss\", torch.mean(losses_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swVrQmuExk-Y"
      },
      "source": [
        "flags['batch_size'] = 64\n",
        "flags['num_workers'] = 0\n",
        "flags['num_epochs'] = 1\n",
        "flags['seed'] = 1234\n",
        "flags['log_steps'] = 10\n",
        "\n",
        "avg_loss_set = []\n",
        "iterations = []\n",
        "time_set = []\n",
        "metrics = []\n",
        "\n",
        "losses = []\n",
        "xmp.spawn(map_fn, args=(flags,), nprocs=8, start_method='fork')\n",
        "# for epoch in range(flags['num_epochs']):\n",
        "#   losses = []\n",
        "\n",
        "\n",
        "#   iterations.append(epoch + 1)\n",
        "#   avg_loss_set.append(torch.mena(losses))\n",
        "#   time_set.append(torch.mean(times))\n",
        "#   results = pd.DataFrame({'iterations': iterations, 'losses': avg_loss_set, 'elapsed_time (mins)': time_set})\n",
        "#   results.to_csv(f\"/content/drive/My Drive/kaggle/lyft-motion-prediction-autonomous-vehicle/{cfg['model_params']['model_name']}epoch{epoch}.csv\", index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6D-ERSXxznp"
      },
      "source": [
        "# write_pred_csv(f'/content/drive/My Drive/kaggle/lyft-motion-prediction-autonomous-vehicle/{cfg[\"model_params\"][\"model_name\"]}_submission.csv',\n",
        "#            timestamps=np.concatenate(timestamps),\n",
        "#            track_ids=np.concatenate(agent_ids),\n",
        "#            coords=np.concatenate(future_coords_offsets_pd),\n",
        "#            confs = np.concatenate(confidences_list)\n",
        "#           )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}